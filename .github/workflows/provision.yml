name: Provision
on: workflow_dispatch

jobs:
  apply-cluster:
    name: Provision Cluster
    runs-on: ubuntu-latest
    environment: production
    env:
      TF_CLOUD_ORGANIZATION: ${{ vars.TF_BACKEND_ORG }}
      TF_WORKSPACE: ${{ vars.TF_BACKEND_WORKSPACE_CLUSTER }}
    steps:
      - name: Check out the repo
        uses: actions/checkout@v3

      - uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.4.6
          cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}

      - name: Init Terraform
        working-directory: ./deploy/terraform/prod/cluster
        run: |
          terraform init -input=false

      - name: Apply
        working-directory: ./deploy/terraform/prod/cluster
        run: |
          terraform apply -input=false -auto-approve \
            -var="do_token=${{ secrets.TF_VAR_DO_DEPLOY_TOKEN }}" \
            -var="cloudflare_token=${{ secrets.TF_VAR_CLOUDFLARE_TOKEN }}" \
            -var="cloudflare_zone_id=${{ vars.TF_VAR_CLOUDFLARE_ZONE_ID }}" \
            -var="do_token_cloud_autoconnect=${{ secrets.TF_VAR_DO_TOKEN_AUTOCONNECT }}" \
            -var="region=${{ vars.TF_VAR_REGION }}" \
            -var="datacenter=${{ vars.TF_VAR_DATACENTER }}" \
            -var="github_actions_repo=${{ github.repository }}" \
            -var="github_actions_pat=${{ secrets.ACTIONS_RUNNER_PAT }}"

  # We have to prempopulate runner cache because we're deploying everything to a single VM.
  # Cold start on a runner can consume all CPU and node becomes unresponsive.
  # Ideally, the runner should have it's own dedicated VM.
  precache-runner:
    name: Prepare Github Runner Cache
    runs-on: self-hosted
    needs: apply-cluster
    environment: production
    steps:
      - uses: actions/checkout@v3

      - uses: actions/setup-go@v4
        with:
          go-version: "^1.20"

      - name: Install golang-migrate
        run: go install -tags 'mongodb' github.com/golang-migrate/migrate/v4/cmd/migrate@v4.14.1

  setup-cluster:
    name: Setup Cluster
    needs: precache-runner
    runs-on: self-hosted
    environment: production
    steps:
      - name: Check Consul
        run: |
          # Wrapping the script in bash to avoid pipefail (Actions run with `-e` by defulat).
          bash -c '
            max_attempts=10
            attempt=1

            while [[ $attempt -le $max_attempts ]]; do
              consul info
              if [[ $? -eq 0 ]]; then
                break
              fi
              
              echo "Consul check failed. Retrying..."
              attempt=$((attempt+1))
              sleep 10
            done

            if [[ $attempt -gt $max_attempts ]]; then
              echo "Max attempts reached. Exiting..."
              exit 1
            fi
          '

      - name: Check Nomad
        run: |
          # Wrapping the script in bash to avoid pipefail (Actions run with `-e` by defulat).
          bash -c '
            max_attempts=10
            attempt=1

            while [[ $attempt -le $max_attempts ]]; do
              nomad status
              if [[ $? -eq 0 ]]; then
                break
              fi
              
              echo "Nomad check failed. Retrying..."
              attempt=$((attempt+1))
              sleep 10
            done

            if [[ $attempt -gt $max_attempts ]]; then
              echo "Max attempts reached. Exiting..."
              exit 1
            fi
          '

      # Have to oversubscribe memory because deploying to a constrained (cheap) environment.
      # https://developer.hashicorp.com/nomad/tutorials/advanced-scheduling/memory-oversubscription
      - name: Enable Nomad Memory oversubscription
        run: |
          curl -s http://localhost:4646/v1/operator/scheduler/configuration | \
            jq '.SchedulerConfig | .MemoryOversubscriptionEnabled=true' | \
            curl -X PUT http://localhost:4646/v1/operator/scheduler/configuration -d @-

  apply-consul:
    name: Apply Consul
    needs: setup-cluster
    runs-on: self-hosted
    environment: production
    steps:
      - name: Check out the repo
        uses: actions/checkout@v3

      - uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.4.6

      - name: Init Terraform
        working-directory: ./deploy/terraform/prod/consul
        run: |
          terraform init -input=false \
            -backend-config="address=localhost:8500" \
            -backend-config="path=terraform/consul/state" \
            -backend-config="scheme=http"

      - name: Apply
        working-directory: ./deploy/terraform/prod/consul
        run: |
          terraform apply -input=false -auto-approve \
            -var="consul_host=localhost:8500" \
            -var="datacenter=${{ vars.TF_VAR_DATACENTER }}" \
            -var="mailersend_token=${{ secrets.TF_VAR_MAILERSEND_TOKEN }}" \
            -var="domain=${{ vars.TF_VAR_DOMAIN }}" \
            -var="cert_email=${{ vars.TF_VAR_CERT_EMAIL }}" \
            -var="google_client_id=${{ vars.TF_VAR_GOOGLE_CLIENT_ID }}" \
            -var="google_client_secret=${{ secrets.TF_VAR_GOOGLE_CLIENT_SECRET }}" \
            -var="storage_access_key=${{ vars.TF_VAR_STORAGE_ACCESS_KEY }}" \
            -var="storage_secret_key=${{ secrets.TF_VAR_STORAGE_SECRET_KEY }}"

  deploy-minio:
    name: Deploy Minio
    needs: apply-consul
    runs-on: self-hosted
    environment: production
    steps:
      - uses: actions/checkout@v3

      - name: Deploy
        working-directory: ./deploy/nomad
        run: nomad job run minio.nomad.hcl

  setup-minio:
    name: Setup Minio
    needs: deploy-minio
    runs-on: ubuntu-latest
    environment: production
    container:
      image: minio/mc:RELEASE.2023-08-08T17-23-59Z

    steps:
      - name: Wait for Minio
        run: |
          until $(curl --output /dev/null --silent --head --fail https://storage.confa.io/minio/health/live); do
              echo 'Waiting for minio...'
              sleep 5
          done
          echo 'minio ready!'

      - name: Create Alias
        run: mc alias set confa https://storage.confa.io ${{ vars.TF_VAR_STORAGE_ACCESS_KEY }} ${{ secrets.TF_VAR_STORAGE_SECRET_KEY }}

      - name: Create Buckets
        run: mc mb -p confa/user-uploads confa/user-public confa/confa-tracks-internal confa/confa-tracks-public

      - name: Set User Public Policy
        run: mc anonymous set download confa/user-public

      - name: Set Tracks Public Policy
        run: mc anonymous set download confa/confa-tracks-public

  deploy-mongo:
    name: Deploy Mongo
    needs: apply-consul
    runs-on: self-hosted
    environment: production
    steps:
      - uses: actions/checkout@v3

      - name: Deploy
        working-directory: ./deploy/nomad
        run: nomad job run mongodb.nomad.hcl

      - name: Init replica set
        run: |
          nomad exec -job mongodb mongo --quiet -u admin -p admin --eval '
            rs.initiate({
              _id: "rs",
              members: [
                {_id: 0, host : "mongodb.service.consul:27017"},
              ]
            })
          '

      - name: Create Users
        run: |
          nomad exec -job mongodb mongo --quiet -u admin -p admin --eval '
            db = db.getSiblingDB("iam")
            if (db.getUser("iam") === null) {
              db.createUser({
                user: "iam",
                pwd: "iam",
                roles: [
                  {
                    role: "readWrite",
                    db: "iam"
                  },
                ]
              })
            }
            
            db = db.getSiblingDB("rtc")
            if (db.getUser("rtc") === null) {
              db.createUser({
                user: "rtc",
                pwd: "rtc",
                roles: [
                  {
                    role: "readWrite",
                    db: "rtc"
                  },
                ]
              })
            }
            
            db = db.getSiblingDB("confa")
            if (db.getUser("confa") === null) {
              db.createUser({
                user: "confa",
                pwd: "confa",
                roles: [
                  {
                    role: "readWrite",
                    db: "confa"
                  },
                ]
              })
            }
          '

  migrate-mongo:
    name: Migrate Mongo
    runs-on: self-hosted
    needs: deploy-mongo
    environment: production
    steps:
      - uses: actions/checkout@v3

      - uses: actions/setup-go@v4
        with:
          go-version: "^1.20"

      - name: Install golang-migrate
        run: go install -tags 'mongodb' github.com/golang-migrate/migrate/v4/cmd/migrate@v4.14.1

      - name: Run migrations
        working-directory: ./services/migrations
        run: |
          migrate -source file://iam/ -database "mongodb://iam:iam@mongodb.service.consul/iam?replicaSet=rs" up
          migrate -source file://rtc/ -database "mongodb://rtc:rtc@mongodb.service.consul/rtc?replicaSet=rs" up
          migrate -source file://confa/ -database "mongodb://confa:confa@mongodb.service.consul/confa?replicaSet=rs" up

  deploy-all:
    name: Deploy all services
    needs: apply-consul
    runs-on: self-hosted
    environment: production
    steps:
      - uses: actions/checkout@v3

      - name: Deploy Teleport
        working-directory: ./deploy/nomad
        run: nomad job run teleport.nomad.hcl

      - name: Configure Github Auth for Teleport
        run: |
          echo '
            version: v3
            kind: github
            metadata:
              name: github
            spec:
              client_id: ${{ vars.TELEPORT_GITHUB_CLIENT_ID }}
              client_secret: ${{ secrets.TELEPORT_GITHUB_CLIENT_SECRET }}
              display: GitHub
              redirect_url: https://teleport.confa.io/v1/webapi/github/callback
              teams_to_roles:
                - organization: ${{ vars.TELEPORT_GITHUB_ORG }}
                  team: ${{ vars.TELEPORT_GITHUB_TEAM }}
                  roles:
                    - access
                    - editor
          ' | nomad exec -job teleport tctl create -f -c /etc/teleport/teleport.yaml

      - name: Deploy all services
        working-directory: ./deploy/nomad
        run: |
          nomad job run -detach livekit.nomad.hcl
          nomad job run -detach traefik.nomad.hcl
          nomad job run -detach tracker.nomad.hcl
          nomad job run -detach beanstalk.nomad.hcl
          nomad job run -detach iam.nomad.hcl
          nomad job run -detach web.nomad.hcl
          nomad job run -detach gateway.nomad.hcl
          nomad job run -detach sender.nomad.hcl
          nomad job run -detach confa.nomad.hcl
          nomad job run -detach rtc.nomad.hcl
          nomad job run -detach avp.nomad.hcl
